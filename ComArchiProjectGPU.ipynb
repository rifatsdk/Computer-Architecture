{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install pynvml\n",
        "!pip install psutil\n",
        "!pip install Pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ijaT-kyG-t1",
        "outputId": "6c6fce7b-c40f-4504-ce84-d0a637326602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting pynvml\n",
            "  Downloading pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting nvidia-ml-py<13.0.0a0,>=12.0.0 (from pynvml)\n",
            "  Downloading nvidia_ml_py-12.560.30-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
            "Downloading nvidia_ml_py-12.560.30-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-ml-py, pynvml\n",
            "Successfully installed nvidia-ml-py-12.560.30 pynvml-12.0.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2hBw19xGy1g",
        "outputId": "d3f8b6ab-5b4a-4921-a681-021abb151e3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiments completed. Results logged to: energy_measurements.csv\n"
          ]
        }
      ],
      "source": [
        "# \"\"\"\n",
        "# Energy Consumption and Optimization of LLM Models in Training and Inference\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "# Project Motivation:\n",
        "# - Rising demand for LLM models and associated operational cost management.\n",
        "# - Lack of energy efficiency insights and sustainability concerns.\n",
        "# - Need for systematic measurement and comparison of energy usage and emissions.\n",
        "\n",
        "# This project involves:\n",
        "# 1. Experimental Setup:\n",
        "#    - We will run experiments on Bridges2 or Google Colab (GPU environment) to measure:\n",
        "#      - Inference energy/time/emissions for GPT-2 (LLM)\n",
        "#      - Training energy/time/emissions for GPT-2 (small training iteration)\n",
        "#      - Inference energy/time/emissions for an image recognition model (ResNet50)\n",
        "#      - Inference energy/time/emissions for a sentence classification model (DistilBERT)\n",
        "\n",
        "# 2. Model Selection and Experiments:\n",
        "#    - GPT-2 for LLM inference and minimal training.\n",
        "#    - ResNet50 (pretrained on ImageNet) for image recognition inference.\n",
        "#    - DistilBERT (fine-tuned on SST-2) for sentence classification inference.\n",
        "\n",
        "# 3. Data Collection and Metrics:\n",
        "#    - Metrics: GPU power draw (Watts), CPU usage, runtime (s), estimated energy (Joules/Wh), CO2 emissions estimate.\n",
        "#    - Tools: `pynvml` for GPU metrics, `psutil` for CPU metrics, timing for runtime.\n",
        "#    - Energy calculation: Energy (Wh) = (Power (W) * Time (h))\n",
        "#      Convert to Joules: 1 Wh = 3600 J.\n",
        "#    - Emissions: Approximate using a carbon intensity factor (e.g., 0.4 kg CO2/kWh).\n",
        "#      This is a rough approximation.\n",
        "\n",
        "# 4. Data Analysis:\n",
        "#    - Compare inference and training steps across models.\n",
        "#    - Identify which model/process consumes more energy and produces more emissions.\n",
        "#    - Propose optimization strategies.\n",
        "\n",
        "# 5. Optimization Recommendations:\n",
        "#    - Mixed precision inference.\n",
        "#    - Model distillation.\n",
        "#    - Efficient fine-tuning methods (e.g., LoRA, adapter training).\n",
        "#    - Hardware-specific optimizations.\n",
        "\n",
        "# Expected Results and Estimates:\n",
        "# - LLM Inference (GPT-2): Fast, moderate energy, low emissions.\n",
        "# - LLM Training: Energy-intensive, moderate emissions.\n",
        "# - Image Recognition Inference (ResNet50): Quick, efficient, low emissions.\n",
        "# - Sentence Classification Inference (DistilBERT): Fast, manageable energy and emissions.\n",
        "\n",
        "# Note:\n",
        "# - The code below is a representative template. Actual hardware, environment setup,\n",
        "#   and credentials (like huggingface token if needed) must be provided.\n",
        "# - Actual GPU/CPU metrics require a GPU environment with `nvidia-smi` or `pynvml` installed.\n",
        "# - This code uses mock carbon intensity and approximations for demonstration purposes.\n",
        "\n",
        "# Instructions:\n",
        "# - Make sure `transformers`, `torch`, `pynvml`, `psutil`, `Pillow`, and `datasets` are installed.\n",
        "# - Run on a GPU-enabled environment for accurate measurements.\n",
        "# \"\"\"\n",
        "\n",
        "# import time\n",
        "# import psutil\n",
        "# import torch\n",
        "# import torchvision.models as models\n",
        "# import torchvision.transforms as transforms\n",
        "# from PIL import Image\n",
        "# import requests\n",
        "# import io\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
        "# from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "# import pynvml\n",
        "# import csv\n",
        "# import os\n",
        "\n",
        "# ###############################\n",
        "# # Configuration and Setup\n",
        "# ###############################\n",
        "\n",
        "# # Energy/Emissions constants (approximations)\n",
        "# CARBON_INTENSITY = 0.4  # kg CO2 per kWh (example factor)\n",
        "# # We'll record data to a CSV file\n",
        "# RESULTS_CSV = \"energy_measurements.csv\"\n",
        "\n",
        "# # Make sure we have GPU\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # Initialize GPU metrics\n",
        "# pynvml.nvmlInit()\n",
        "# gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
        "\n",
        "# ###############################\n",
        "# # Utility Functions\n",
        "# ###############################\n",
        "\n",
        "# def get_gpu_power_watts():\n",
        "#     \"\"\"Get current GPU power usage in watts.\"\"\"\n",
        "#     pinfo = pynvml.nvmlDeviceGetPowerUsage(gpu_handle)\n",
        "#     # power usage in milliwatts, convert to watts\n",
        "#     return pinfo / 1000.0\n",
        "\n",
        "# def get_cpu_power_estimate():\n",
        "#     \"\"\"Estimate CPU power usage. This is a rough approximation using CPU utilization.\"\"\"\n",
        "#     # Without specialized instrumentation or RAPL, exact CPU power is hard to get.\n",
        "#     # We'll just record CPU utilization as a proxy.\n",
        "#     cpu_util = psutil.cpu_percent()\n",
        "#     # A rough estimate: Suppose full CPU usage ~50W for the system.\n",
        "#     # This is a guess and should be replaced with real hardware-specific measurement.\n",
        "#     estimated_cpu_power = (cpu_util / 100.0) * 50\n",
        "#     return estimated_cpu_power\n",
        "\n",
        "# def measure_energy_and_time(func, *args, **kwargs):\n",
        "#     \"\"\"\n",
        "#     Measure the runtime and approximate energy usage for a given function.\n",
        "#     We'll measure GPU and CPU power multiple times and integrate over the runtime.\n",
        "#     \"\"\"\n",
        "#     start_time = time.time()\n",
        "#     energy_gpu_joules = 0.0\n",
        "#     energy_cpu_joules = 0.0\n",
        "\n",
        "#     # We'll sample power usage at intervals\n",
        "#     interval = 0.1  # 100ms sampling\n",
        "#     running = True\n",
        "#     import threading\n",
        "\n",
        "#     def target():\n",
        "#         func(*args, **kwargs)\n",
        "\n",
        "#     thread = threading.Thread(target=target)\n",
        "#     thread.start()\n",
        "\n",
        "#     while thread.is_alive():\n",
        "#         gpu_watts = get_gpu_power_watts()\n",
        "#         cpu_watts = get_cpu_power_estimate()\n",
        "#         time.sleep(interval)\n",
        "#         energy_gpu_joules += gpu_watts * (interval) * 1.0  # W * s = J\n",
        "#         energy_cpu_joules += cpu_watts * (interval) * 1.0  # W * s = J\n",
        "\n",
        "#     thread.join()\n",
        "#     end_time = time.time()\n",
        "#     total_time = end_time - start_time\n",
        "#     total_energy_joules = energy_gpu_joules + energy_cpu_joules\n",
        "\n",
        "#     # Convert joules to kWh: 1 J = 2.77778e-7 kWh\n",
        "#     total_energy_kwh = total_energy_joules * 2.77778e-7\n",
        "\n",
        "#     # Emissions:\n",
        "#     # Emissions (kg CO2) = total_energy_kwh * CARBON_INTENSITY\n",
        "#     emissions_kg = total_energy_kwh * CARBON_INTENSITY\n",
        "\n",
        "#     return total_time, total_energy_joules, emissions_kg\n",
        "\n",
        "# def log_results(task_name, model_name, phase, total_time, energy_joules, emissions_kg):\n",
        "#     file_exists = os.path.isfile(RESULTS_CSV)\n",
        "#     with open(RESULTS_CSV, 'a', newline='') as f:\n",
        "#         writer = csv.writer(f)\n",
        "#         if not file_exists:\n",
        "#             writer.writerow([\"Task\", \"Model\", \"Phase\", \"Time(s)\", \"Energy(J)\", \"Emissions(kgCO2)\"])\n",
        "#         writer.writerow([task_name, model_name, phase, total_time, energy_joules, emissions_kg])\n",
        "\n",
        "# ###############################\n",
        "# # Models and Experiments Setup\n",
        "# ###############################\n",
        "\n",
        "# # 1. LLM: GPT-2 Inference and minimal training demonstration\n",
        "# llm_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "# llm_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
        "# llm_model.eval()\n",
        "\n",
        "# # 2. Image Recognition: ResNet50\n",
        "# resnet_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2).to(device)\n",
        "# resnet_model.eval()\n",
        "# img_transform = transforms.Compose([\n",
        "#     transforms.Resize((224,224)),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485,0.456,0.406],\n",
        "#                          std=[0.229,0.224,0.225]),\n",
        "# ])\n",
        "\n",
        "# # 3. Sentence Classification: DistilBERT (SST-2)\n",
        "# distil_bert_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "# distil_bert_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\").to(device)\n",
        "# distil_bert_model.eval()\n",
        "\n",
        "# ###############################\n",
        "# # Example Inputs\n",
        "# ###############################\n",
        "\n",
        "# # LLM inference input\n",
        "# llm_input_text = \"The future of energy-efficient AI models is\"\n",
        "# llm_inputs = llm_tokenizer(llm_input_text, return_tensors='pt').to(device)\n",
        "\n",
        "# def llm_inference():\n",
        "#     with torch.no_grad():\n",
        "#         outputs = llm_model.generate(**llm_inputs, max_length=50)\n",
        "\n",
        "# # LLM training setup (small dummy training step)\n",
        "# # We'll create a tiny random dataset of next-token prediction for demonstration.\n",
        "# def llm_training_step():\n",
        "#     llm_model.train()\n",
        "#     optimizer = torch.optim.AdamW(llm_model.parameters(), lr=1e-5)\n",
        "#     dummy_input_ids = torch.randint(0, llm_model.config.vocab_size, (2, 16)).to(device) # batch 2, seq 16\n",
        "#     dummy_labels = dummy_input_ids.clone()\n",
        "#     outputs = llm_model(dummy_input_ids, labels=dummy_labels)\n",
        "#     loss = outputs.loss\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "\n",
        "# # Image Recognition Inference input\n",
        "# img_pil = Image.open(\"dog.jpg\")\n",
        "# img_tensor = img_transform(img_pil).unsqueeze(0).to(device)\n",
        "\n",
        "# def image_inference():\n",
        "#     with torch.no_grad():\n",
        "#         res = resnet_model(img_tensor)\n",
        "#         res = resnet_model(img_tensor)\n",
        "\n",
        "# # Sentence Classification Inference input\n",
        "# sent_input_text = \"I love this movie!\"\n",
        "# sent_inputs = distil_bert_tokenizer(sent_input_text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "\n",
        "# def sentence_inference():\n",
        "#     with torch.no_grad():\n",
        "#         out = distil_bert_model(**sent_inputs)\n",
        "\n",
        "# ###############################\n",
        "# # Run Experiments and Record\n",
        "# ###############################\n",
        "\n",
        "# # LLM Inference\n",
        "# time_, energy_j, emissions_kg = measure_energy_and_time(llm_inference)\n",
        "# log_results(\"LLM Inference\", \"GPT-2\", \"Inference\", time_, energy_j, emissions_kg)\n",
        "\n",
        "# # LLM Training (just a single step demo, in practice you'd loop multiple steps)\n",
        "# time_, energy_j, emissions_kg = measure_energy_and_time(llm_training_step)\n",
        "# log_results(\"LLM Training\", \"GPT-2\", \"Training\", time_, energy_j, emissions_kg)\n",
        "\n",
        "# # Image Recognition Inference\n",
        "# time_, energy_j, emissions_kg = measure_energy_and_time(image_inference)\n",
        "# log_results(\"Image Recognition Inference\", \"ResNet50\", \"Inference\", time_, energy_j, emissions_kg)\n",
        "\n",
        "# # Sentence Classification Inference\n",
        "# time_, energy_j, emissions_kg = measure_energy_and_time(sentence_inference)\n",
        "# log_results(\"Sentence Classification Inference\", \"DistilBERT\", \"Inference\", time_, energy_j, emissions_kg)\n",
        "\n",
        "# ###############################\n",
        "# # Optimization Example (Optional)\n",
        "# ###############################\n",
        "\n",
        "# # For example, try mixed-precision inference for GPT-2\n",
        "# # This will reduce energy/time slightly, we can measure again\n",
        "# llm_model_fp16 = AutoModelForCausalLM.from_pretrained(\"gpt2\", torch_dtype=torch.float16).to(device)\n",
        "# llm_model_fp16.eval()\n",
        "\n",
        "# def llm_inference_fp16():\n",
        "#     with torch.no_grad():\n",
        "#         outputs = llm_model_fp16.generate(**llm_inputs, max_length=50)\n",
        "\n",
        "# time_, energy_j, emissions_kg = measure_energy_and_time(llm_inference_fp16)\n",
        "# log_results(\"LLM Inference Optimized\", \"GPT-2 FP16\", \"Inference\", time_, energy_j, emissions_kg)\n",
        "\n",
        "# ###############################\n",
        "# # Further steps (not coded):\n",
        "# # - Analyze results in CSV\n",
        "# # - Plot energy/time tradeoffs\n",
        "# # - Provide recommendations\n",
        "# ###############################\n",
        "\n",
        "# print(\"Experiments completed. Results logged to:\", RESULTS_CSV)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # -*- coding: utf-8 -*-\n",
        "# \"\"\"GPT2_Transfer Learning with Energy Measurement\n",
        "\n",
        "# This code:\n",
        "# - Performs text classification on tweets (sentiment analysis) using GPT-2 as a feature extractor.\n",
        "# - Uses a subset of the data for quicker experimentation.\n",
        "# - Measures energy consumption (GPU & CPU) and execution time for training and evaluation steps.\n",
        "# - Logs results (time, energy, emissions) into a CSV file.\n",
        "\n",
        "# \"\"\"\n",
        "\n",
        "# # !pip install transformers psutil pynvml\n",
        "\n",
        "# import math\n",
        "# import tensorflow as tf\n",
        "# from transformers import GPT2Tokenizer, TFGPT2Model\n",
        "# from google.colab import drive\n",
        "# import pandas as pd\n",
        "\n",
        "# import time\n",
        "# import psutil\n",
        "# import torch\n",
        "# import pynvml\n",
        "# import threading\n",
        "# import csv\n",
        "# import os\n",
        "\n",
        "# ###############################\n",
        "# # Mount Google Drive\n",
        "# ###############################\n",
        "# # drive.mount('/content/gdrive')\n",
        "\n",
        "# ###############################\n",
        "# # Energy/Emissions constants (approximations)\n",
        "# ###############################\n",
        "# CARBON_INTENSITY = 0.4  # kg CO2 per kWh (example factor)\n",
        "# RESULTS_CSV1 = \"energy_measurements1.csv\"\n",
        "\n",
        "# ###############################\n",
        "# # GPU Initialization for Energy Measurement\n",
        "# ###############################\n",
        "# pynvml.nvmlInit()\n",
        "# gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
        "\n",
        "# def get_gpu_power_watts():\n",
        "#     \"\"\"Get current GPU power usage in watts.\"\"\"\n",
        "#     pinfo = pynvml.nvmlDeviceGetPowerUsage(gpu_handle)\n",
        "#     return pinfo / 1000.0\n",
        "\n",
        "# def get_cpu_power_estimate():\n",
        "#     \"\"\"\n",
        "#     Estimate CPU power usage.\n",
        "#     This is a rough approximation.\n",
        "#     Adjust based on your hardware or use RAPL if available.\n",
        "#     \"\"\"\n",
        "#     cpu_util = psutil.cpu_percent()\n",
        "#     # Assume full CPU usage ~50W for the system\n",
        "#     estimated_cpu_power = (cpu_util / 100.0) * 50\n",
        "#     return estimated_cpu_power\n",
        "\n",
        "# def measure_energy_and_time(func, *args, **kwargs):\n",
        "#     \"\"\"\n",
        "#     Measure runtime and approximate energy usage for a given function call.\n",
        "#     Sampling GPU/CPU usage every 100ms.\n",
        "#     \"\"\"\n",
        "#     start_time = time.time()\n",
        "#     energy_gpu_joules = 0.0\n",
        "#     energy_cpu_joules = 0.0\n",
        "#     interval = 0.1\n",
        "\n",
        "#     def target():\n",
        "#         func(*args, **kwargs)\n",
        "\n",
        "#     thread = threading.Thread(target=target)\n",
        "#     thread.start()\n",
        "\n",
        "#     while thread.is_alive():\n",
        "#         gpu_watts = get_gpu_power_watts()\n",
        "#         cpu_watts = get_cpu_power_estimate()\n",
        "#         time.sleep(interval)\n",
        "#         energy_gpu_joules += gpu_watts * interval\n",
        "#         energy_cpu_joules += cpu_watts * interval\n",
        "\n",
        "#     thread.join()\n",
        "#     end_time = time.time()\n",
        "#     total_time = end_time - start_time\n",
        "#     total_energy_joules = energy_gpu_joules + energy_cpu_joules\n",
        "#     total_energy_kwh = total_energy_joules * 2.77778e-7\n",
        "#     emissions_kg = total_energy_kwh * CARBON_INTENSITY\n",
        "\n",
        "#     return total_time, total_energy_joules, emissions_kg\n",
        "\n",
        "# def log_results(task_name, phase, total_time, energy_joules, emissions_kg, filename=RESULTS_CSV):\n",
        "#     file_exists = os.path.isfile(filename)\n",
        "#     with open(filename, 'a', newline='') as f:\n",
        "#         writer = csv.writer(f)\n",
        "#         if not file_exists:\n",
        "#             writer.writerow([\"Task\", \"Phase\", \"Time(s)\", \"Energy(J)\", \"Emissions(kgCO2)\"])\n",
        "#         writer.writerow([task_name, phase, total_time, energy_joules, emissions_kg])\n",
        "\n",
        "# ###############################\n",
        "# # Load Data\n",
        "# ###############################\n",
        "# # Ensure that your data file is accessible in your drive\n",
        "# # Adjust the path if needed.\n",
        "# df = pd.read_csv('tweets_processed.csv')\n",
        "\n",
        "# # Use a subset of data for quicker tests (e.g., 10% of original data)\n",
        "# df = df.sample(frac=0.1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X = df['lemmatized_tweets']\n",
        "# y = df['VADER_sentiment']\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
        "\n",
        "# ###############################\n",
        "# # Preprocessing and Tokenization\n",
        "# ###############################\n",
        "# MAX_LENGTH = math.ceil((X_train.apply(lambda x: len(str(x).split())).mean()))+2\n",
        "\n",
        "# PAD_TOKEN = \"<|pad|>\"\n",
        "# EOS_TOKEN = \"<|endoftext|>\"\n",
        "\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\",\n",
        "#     pad_token=PAD_TOKEN,\n",
        "#     eos_token=EOS_TOKEN,\n",
        "#     max_length=MAX_LENGTH,\n",
        "#     is_split_into_words=True)\n",
        "\n",
        "# X_train = [str(ex) + EOS_TOKEN for ex in X_train]\n",
        "# X_test = [str(ex) + EOS_TOKEN for ex in X_test]\n",
        "\n",
        "# X_train_ = [tokenizer(str(x), return_tensors='tf', max_length=MAX_LENGTH,\n",
        "#                       truncation=True, padding=\"max_length\", add_special_tokens=True)['input_ids'] for x in X_train]\n",
        "# X_test_ = [tokenizer(str(x), return_tensors='tf', max_length=MAX_LENGTH,\n",
        "#                      truncation=True, padding=\"max_length\", add_special_tokens=True)['input_ids'] for x in X_test]\n",
        "\n",
        "# X_train_in = tf.squeeze(tf.convert_to_tensor(X_train_), axis=1)\n",
        "# X_test_in = tf.squeeze(tf.convert_to_tensor(X_test_), axis=1)\n",
        "\n",
        "# X_train_mask_ = [tokenizer(str(x), return_tensors='tf', max_length=MAX_LENGTH,\n",
        "#                            truncation=True, padding=\"max_length\", add_special_tokens=True)[\"attention_mask\"] for x in X_train]\n",
        "# X_test_mask_ = [tokenizer(str(x), return_tensors='tf', max_length=MAX_LENGTH,\n",
        "#                           truncation=True, padding=\"max_length\", add_special_tokens=True)[\"attention_mask\"] for x in X_test]\n",
        "\n",
        "# X_train_mask = tf.squeeze(tf.convert_to_tensor(X_train_mask_), axis=1)\n",
        "# X_test_mask = tf.squeeze(tf.convert_to_tensor(X_test_mask_), axis=1)\n",
        "\n",
        "# ###############################\n",
        "# # Model Setup\n",
        "# ###############################\n",
        "# model = TFGPT2Model.from_pretrained(\"gpt2\", use_cache=False,\n",
        "#         pad_token_id=tokenizer.pad_token_id,\n",
        "#         eos_token_id=tokenizer.eos_token_id)\n",
        "# model.training = True\n",
        "# model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# # Freeze GPT2 layers\n",
        "# for layer in model.layers:\n",
        "#     layer.trainable = False\n",
        "\n",
        "# input_ = tf.keras.layers.Input(shape=(None,), dtype='int32')\n",
        "# mask_ = tf.keras.layers.Input(shape=(None,), dtype='int32')\n",
        "# x = model(input_, attention_mask=mask_)\n",
        "# x = tf.reduce_mean(x.last_hidden_state, axis=1)\n",
        "# x = tf.keras.layers.Dense(16, activation='relu')(x)\n",
        "# x = tf.keras.layers.Dropout(0.3)(x)\n",
        "# output = tf.keras.layers.Dense(3, activation='softmax')(x)\n",
        "\n",
        "# clf = tf.keras.Model([input_, mask_], output)\n",
        "\n",
        "# base_learning_rate = 0.0005\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate)\n",
        "# loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "# clf.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "# callbacks = tf.keras.callbacks.EarlyStopping(\n",
        "#     monitor=\"accuracy\", verbose=1, patience=3, restore_best_weights=True)\n",
        "\n",
        "# def map_sentiment(value):\n",
        "#   if value == 'Negative':\n",
        "#     return 0\n",
        "#   if value == 'Neutral':\n",
        "#     return 1\n",
        "#   if value == 'Positive':\n",
        "#     return 2\n",
        "\n",
        "# y_train_ = y_train.map(map_sentiment)\n",
        "# y_test_ = y_test.map(map_sentiment)\n",
        "\n",
        "# y_train_in = tf.constant(y_train_, dtype=tf.int32)\n",
        "# y_test_in = tf.constant(y_test_, dtype=tf.int32)\n",
        "\n",
        "# tf.config.experimental_run_functions_eagerly(True)\n",
        "\n",
        "# def train_model():\n",
        "#     clf.fit([X_train_in, X_train_mask], y_train_in, epochs=5, batch_size=32, validation_split=0.2, callbacks=callbacks)\n",
        "\n",
        "# ###############################\n",
        "# # Training with Energy Measurement\n",
        "# ###############################\n",
        "# print(\"Starting training...\")\n",
        "# train_time, train_energy_j, train_emissions_kg = measure_energy_and_time(train_model)\n",
        "# print(f\"Training completed in {train_time:.2f}s, Energy: {train_energy_j:.2f}J, Emissions: {train_emissions_kg:.4f}kg CO2\")\n",
        "# log_results(\"GPT2_TransferLearning\", \"Training\", train_time, train_energy_j, train_emissions_kg)\n",
        "\n",
        "# ###############################\n",
        "# # Evaluation with Energy Measurement\n",
        "# ###############################\n",
        "# print(\"Evaluating on test set...\")\n",
        "# eval_time, eval_energy_j, eval_emissions_kg = measure_energy_and_time(clf.evaluate, [X_test_in, X_test_mask], y_test_in)\n",
        "# print(f\"Evaluation completed in {eval_time:.2f}s, Energy: {eval_energy_j:.2f}J, Emissions: {eval_emissions_kg:.4f}kg CO2\")\n",
        "# log_results(\"GPT2_TransferLearning\", \"Evaluation\", eval_time, eval_energy_j, eval_emissions_kg)\n",
        "\n",
        "# clf.training = False\n",
        "# y_pred = clf.predict([X_test_in, X_test_mask])\n",
        "\n",
        "# print(\"Done. Results saved to:\", RESULTS_CSV1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng7msj17HcLg",
        "outputId": "bffc9c7a-c40e-4555-b2a8-6c542dc65709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2Model.\n",
            "\n",
            "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 3s 925ms/step - loss: 11.2003 - accuracy: 0.4113 - val_loss: 9.7950 - val_accuracy: 0.2500\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 2s 612ms/step - loss: 11.0644 - accuracy: 0.3952 - val_loss: 8.1811 - val_accuracy: 0.2500\n",
            "Epoch 3/5\n",
            "4/4 [==============================] - 2s 610ms/step - loss: 11.1697 - accuracy: 0.3306 - val_loss: 6.9311 - val_accuracy: 0.2500\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 3s 801ms/step - loss: 7.8770 - accuracy: 0.4839 - val_loss: 5.6655 - val_accuracy: 0.2500\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 3s 752ms/step - loss: 8.5798 - accuracy: 0.4032 - val_loss: 4.4446 - val_accuracy: 0.2812\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "Training completed in 20.63s, Energy: 1235.18J, Emissions: 0.0001kg CO2\n",
            "Evaluating on test set...\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 4.8946 - accuracy: 0.3585\n",
            "Evaluation completed in 0.52s, Energy: 32.15J, Emissions: 0.0000kg CO2\n",
            "2/2 [==============================] - 0s 207ms/step\n",
            "Done. Results saved to: energy_measurements1.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"GPT2_Transfer Learning with Energy Measurement\"\"\"\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install transformers psutil pynvml\n",
        "\n",
        "# Import required libraries\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from transformers import GPT2Tokenizer, TFGPT2Model\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "import time\n",
        "import psutil\n",
        "import threading\n",
        "import csv\n",
        "import os\n",
        "\n",
        "###############################\n",
        "# Constants and Setup\n",
        "###############################\n",
        "CARBON_INTENSITY = 0.4  # kg CO2 per kWh (example factor)\n",
        "RESULTS_CSV = \"energy_measurements_gpu.csv\"\n",
        "USE_GPU = tf.config.list_physical_devices('GPU')  # Check if GPU is available\n",
        "\n",
        "###############################\n",
        "# Mount Google Drive\n",
        "###############################\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "###############################\n",
        "# GPU and CPU Power Functions\n",
        "###############################\n",
        "\n",
        "# Function to Measure Power Usage with nvidia-smi\n",
        "def get_gpu_power():\n",
        "    if USE_GPU:\n",
        "        result = subprocess.run(\n",
        "            [\"nvidia-smi\", \"--query-gpu=power.draw\", \"--format=csv,nounits,noheader\"],\n",
        "            stdout=subprocess.PIPE, text=True)\n",
        "        return float(result.stdout.strip())\n",
        "    return 0.0\n",
        "\n",
        "# Estimate CPU power usage\n",
        "def get_cpu_power_estimate():\n",
        "    \"\"\"\n",
        "    Estimate CPU power usage.\n",
        "    This is a rough approximation.\n",
        "    \"\"\"\n",
        "    cpu_util = psutil.cpu_percent()\n",
        "    estimated_cpu_power = (cpu_util / 100.0) * 50\n",
        "    return estimated_cpu_power\n",
        "\n",
        "# Measure energy consumption and time\n",
        "def measure_energy_and_time(func, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Measure runtime and approximate energy usage for a given function call.\n",
        "    This now includes an alternative method using `nvidia-smi` for GPU power monitoring.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    energy_gpu_joules = 0.0\n",
        "    energy_cpu_joules = 0.0\n",
        "    interval = 0.1  # Sampling interval in seconds\n",
        "\n",
        "    def target():\n",
        "        func(*args, **kwargs)\n",
        "\n",
        "    thread = threading.Thread(target=target)\n",
        "    thread.start()\n",
        "\n",
        "    while thread.is_alive():\n",
        "        # Measure GPU power using nvidia-smi or pynvml\n",
        "        gpu_watts = get_gpu_power()\n",
        "        cpu_watts = get_cpu_power_estimate()\n",
        "        time.sleep(interval)\n",
        "        energy_gpu_joules += gpu_watts * interval\n",
        "        energy_cpu_joules += cpu_watts * interval\n",
        "\n",
        "    thread.join()\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    total_energy_joules = energy_gpu_joules + energy_cpu_joules\n",
        "    total_energy_kwh = total_energy_joules * 2.77778e-7\n",
        "    emissions_kg = total_energy_kwh * CARBON_INTENSITY\n",
        "\n",
        "    return total_time, total_energy_joules, emissions_kg\n",
        "\n",
        "# Logging function\n",
        "def log_results(task_name, phase, total_time, energy_joules, emissions_kg, filename=RESULTS_CSV):\n",
        "    file_exists = os.path.isfile(filename)\n",
        "    with open(filename, 'a', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        if not file_exists:\n",
        "            writer.writerow([\"Task\", \"Phase\", \"Time(s)\", \"Energy(J)\", \"Emissions(kgCO2)\"])\n",
        "        writer.writerow([task_name, phase, total_time, energy_joules, emissions_kg])\n",
        "\n",
        "###############################\n",
        "# Load Data\n",
        "###############################\n",
        "# Ensure that your data file is accessible in your drive\n",
        "df = pd.read_csv('tweets_processed.csv')\n",
        "\n",
        "# Use a subset of data for quicker tests (e.g., 10% of original data)\n",
        "df = df.sample(frac=0.1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df['lemmatized_tweets']\n",
        "y = df['VADER_sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
        "\n",
        "###############################\n",
        "# Preprocessing and Tokenization\n",
        "###############################\n",
        "MAX_LENGTH = math.ceil((X_train.apply(lambda x: len(str(x).split())).mean()))+2\n",
        "\n",
        "PAD_TOKEN = \"<|pad|>\"\n",
        "EOS_TOKEN = \"<|endoftext|>\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\",\n",
        "    pad_token=PAD_TOKEN,\n",
        "    eos_token=EOS_TOKEN,\n",
        "    max_length=MAX_LENGTH,\n",
        "    is_split_into_words=True)\n",
        "\n",
        "X_train = [str(ex) + EOS_TOKEN for ex in X_train]\n",
        "X_test = [str(ex) + EOS_TOKEN for ex in X_test]\n",
        "\n",
        "X_train_ = [tokenizer(str(x), return_tensors='tf', max_length=MAX_LENGTH,\n",
        "                      truncation=True, padding=\"max_length\", add_special_tokens=True)['input_ids'] for x in X_train]\n",
        "X_test_ = [tokenizer(str(x), return_tensors='tf', max_length=MAX_LENGTH,\n",
        "                     truncation=True, padding=\"max_length\", add_special_tokens=True)['input_ids'] for x in X_test]\n",
        "\n",
        "X_train_in = tf.squeeze(tf.convert_to_tensor(X_train_), axis=1)\n",
        "X_test_in = tf.squeeze(tf.convert_to_tensor(X_test_), axis=1)\n",
        "\n",
        "X_train_mask_ = [tokenizer(str(x), return_tensors='tf', max_length=MAX_LENGTH,\n",
        "                           truncation=True, padding=\"max_length\", add_special_tokens=True)[\"attention_mask\"] for x in X_train]\n",
        "X_test_mask_ = [tokenizer(str(x), return_tensors='tf', max_length=MAX_LENGTH,\n",
        "                          truncation=True, padding=\"max_length\", add_special_tokens=True)[\"attention_mask\"] for x in X_test]\n",
        "\n",
        "X_train_mask = tf.squeeze(tf.convert_to_tensor(X_train_mask_), axis=1)\n",
        "X_test_mask = tf.squeeze(tf.convert_to_tensor(X_test_mask_), axis=1)\n",
        "\n",
        "###############################\n",
        "# Model Setup\n",
        "###############################\n",
        "model = TFGPT2Model.from_pretrained(\"gpt2\", use_cache=False,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id)\n",
        "model.training = True\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Freeze GPT2 layers\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "input_ = tf.keras.layers.Input(shape=(None,), dtype='int32')\n",
        "mask_ = tf.keras.layers.Input(shape=(None,), dtype='int32')\n",
        "x = model(input_, attention_mask=mask_)\n",
        "x = tf.reduce_mean(x.last_hidden_state, axis=1)\n",
        "x = tf.keras.layers.Dense(16, activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(0.3)(x)\n",
        "output = tf.keras.layers.Dense(3, activation='softmax')(x)\n",
        "\n",
        "clf = tf.keras.Model([input_, mask_], output)\n",
        "\n",
        "base_learning_rate = 0.0005\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "clf.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"accuracy\", verbose=1, patience=3, restore_best_weights=True)\n",
        "\n",
        "def map_sentiment(value):\n",
        "  if value == 'Negative':\n",
        "    return 0\n",
        "  if value == 'Neutral':\n",
        "    return 1\n",
        "  if value == 'Positive':\n",
        "    return 2\n",
        "\n",
        "y_train_ = y_train.map(map_sentiment)\n",
        "y_test_ = y_test.map(map_sentiment)\n",
        "\n",
        "y_train_in = tf.constant(y_train_, dtype=tf.int32)\n",
        "y_test_in = tf.constant(y_test_, dtype=tf.int32)\n",
        "\n",
        "tf.config.experimental_run_functions_eagerly(True)\n",
        "\n",
        "def train_model():\n",
        "    clf.fit([X_train_in, X_train_mask], y_train_in, epochs=5, batch_size=32, validation_split=0.2, callbacks=callbacks)\n",
        "\n",
        "###############################\n",
        "# Training with Energy Measurement\n",
        "###############################\n",
        "print(\"Starting training...\")\n",
        "train_time, train_energy_j, train_emissions_kg = measure_energy_and_time(train_model)\n",
        "print(f\"Training completed in {train_time:.2f}s, Energy: {train_energy_j:.2f}J, Emissions: {train_emissions_kg:.4f}kg CO2\")\n",
        "log_results(\"GPT2_TransferLearning\", \"Training\", train_time, train_energy_j, train_emissions_kg)\n",
        "\n",
        "###############################\n",
        "# Evaluation with Energy Measurement\n",
        "###############################\n",
        "print(\"Evaluating on test set...\")\n",
        "eval_time, eval_energy_j, eval_emissions_kg = measure_energy_and_time(clf.evaluate, [X_test_in, X_test_mask], y_test_in)\n",
        "print(f\"Evaluation completed in {eval_time:.2f}s, Energy: {eval_energy_j:.2f}J, Emissions: {eval_emissions_kg:.4f}kg CO2\")\n",
        "log_results(\"GPT2_TransferLearning\", \"Evaluation\", eval_time, eval_energy_j, eval_emissions_kg)\n",
        "\n",
        "clf.training = False\n",
        "y_pred = clf.predict([X_test_in, X_test_mask])\n",
        "\n",
        "print(\"Done. Results saved to:\", RESULTS_CSV)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKTi6wwKKL9f",
        "outputId": "8ee037c0-af74-4f29-a9ef-910664044509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (12.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from pynvml) (12.560.30)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2Model.\n",
            "\n",
            "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - ETA: 0s - loss: 6.8715 - accuracy: 0.2984"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4/4 [==============================] - 4s 993ms/step - loss: 6.8715 - accuracy: 0.2984 - val_loss: 8.1579 - val_accuracy: 0.1250\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 3s 775ms/step - loss: 6.4132 - accuracy: 0.3065 - val_loss: 6.4392 - val_accuracy: 0.1250\n",
            "Epoch 3/5\n",
            "4/4 [==============================] - 4s 983ms/step - loss: 5.2451 - accuracy: 0.3145 - val_loss: 4.8973 - val_accuracy: 0.1250\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 3s 750ms/step - loss: 4.5685 - accuracy: 0.3468 - val_loss: 3.6648 - val_accuracy: 0.1250\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 2s 624ms/step - loss: 3.8470 - accuracy: 0.3468 - val_loss: 2.8701 - val_accuracy: 0.2500\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "Training completed in 20.58s, Energy: 1066.56J, Emissions: 0.0001kg CO2\n",
            "Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 1s 344ms/step - loss: 3.0356 - accuracy: 0.2830\n",
            "Evaluation completed in 0.81s, Energy: 49.05J, Emissions: 0.0000kg CO2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 213ms/step\n",
            "Done. Results saved to: energy_measurements.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Frezzing"
      ],
      "metadata": {
        "id": "wz43Ph7rMzds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"GPT2_Transfer Learning with Energy Measurement and GPU Insights\"\"\"\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install transformers psutil pynvml\n",
        "\n",
        "# Import required libraries\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from transformers import GPT2Tokenizer, TFGPT2Model\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "import time\n",
        "import psutil\n",
        "import threading\n",
        "import csv\n",
        "import os\n",
        "import pynvml\n",
        "\n",
        "###############################\n",
        "# Constants and Setup\n",
        "###############################\n",
        "CARBON_INTENSITY = 0.4  # kg CO2 per kWh (example factor)\n",
        "RESULTS_CSV = \"energy_measurements_freze_gpu.csv\"\n",
        "GPU_DETAILS_CSV = \"gpu_details_freze_gpu.csv\"\n",
        "USE_GPU = tf.config.list_physical_devices('GPU')  # Check if GPU is available\n",
        "\n",
        "###############################\n",
        "# GPU and CPU Power Functions\n",
        "###############################\n",
        "\n",
        "def get_gpu_details():\n",
        "    \"\"\"\n",
        "    Retrieve and print GPU architecture and other insights using NVML.\n",
        "    \"\"\"\n",
        "    pynvml.nvmlInit()\n",
        "    device_count = pynvml.nvmlDeviceGetCount()\n",
        "    details = []\n",
        "\n",
        "    for i in range(device_count):\n",
        "        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
        "        name = pynvml.nvmlDeviceGetName(handle)  # No need to decode\n",
        "        memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
        "        utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
        "        architecture = pynvml.nvmlSystemGetCudaDriverVersion()\n",
        "\n",
        "        details.append({\n",
        "            \"GPU\": name,\n",
        "            \"Memory Total (MB)\": memory_info.total // (1024 ** 2),\n",
        "            \"Memory Free (MB)\": memory_info.free // (1024 ** 2),\n",
        "            \"Memory Used (MB)\": memory_info.used // (1024 ** 2),\n",
        "            \"GPU Utilization (%)\": utilization.gpu,\n",
        "            \"Memory Utilization (%)\": utilization.memory,\n",
        "            \"CUDA Version\": architecture\n",
        "        })\n",
        "\n",
        "    pynvml.nvmlShutdown()\n",
        "    return details\n",
        "\n",
        "\n",
        "# Function to Measure Power Usage with nvidia-smi\n",
        "def get_gpu_power():\n",
        "    if USE_GPU:\n",
        "        result = subprocess.run(\n",
        "            [\"nvidia-smi\", \"--query-gpu=power.draw\", \"--format=csv,nounits,noheader\"],\n",
        "            stdout=subprocess.PIPE, text=True)\n",
        "        return float(result.stdout.strip())\n",
        "    return 0.0\n",
        "\n",
        "# Estimate CPU power usage\n",
        "def get_cpu_power_estimate():\n",
        "    \"\"\"\n",
        "    Estimate CPU power usage.\n",
        "    This is a rough approximation.\n",
        "    \"\"\"\n",
        "    cpu_util = psutil.cpu_percent()\n",
        "    estimated_cpu_power = (cpu_util / 100.0) * 50\n",
        "    return estimated_cpu_power\n",
        "\n",
        "# Measure energy consumption and time\n",
        "def measure_energy_and_time(func, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Measure runtime and approximate energy usage for a given function call.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    energy_gpu_joules = 0.0\n",
        "    energy_cpu_joules = 0.0\n",
        "    interval = 0.1  # Sampling interval in seconds\n",
        "\n",
        "    def target():\n",
        "        func(*args, **kwargs)\n",
        "\n",
        "    thread = threading.Thread(target=target)\n",
        "    thread.start()\n",
        "\n",
        "    while thread.is_alive():\n",
        "        # Measure GPU power using nvidia-smi\n",
        "        gpu_watts = get_gpu_power()\n",
        "        cpu_watts = get_cpu_power_estimate()\n",
        "        time.sleep(interval)\n",
        "        energy_gpu_joules += gpu_watts * interval\n",
        "        energy_cpu_joules += cpu_watts * interval\n",
        "\n",
        "    thread.join()\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    total_energy_joules = energy_gpu_joules + energy_cpu_joules\n",
        "    total_energy_kwh = total_energy_joules * 2.77778e-7\n",
        "    emissions_kg = total_energy_kwh * CARBON_INTENSITY\n",
        "\n",
        "    return total_time, total_energy_joules, emissions_kg\n",
        "\n",
        "# Logging functions\n",
        "def log_results(task_name, phase, total_time, energy_joules, emissions_kg, filename=RESULTS_CSV):\n",
        "    file_exists = os.path.isfile(filename)\n",
        "    with open(filename, 'a', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        if not file_exists:\n",
        "            writer.writerow([\"Task\", \"Phase\", \"Time(s)\", \"Energy(J)\", \"Emissions(kgCO2)\"])\n",
        "        writer.writerow([task_name, phase, total_time, energy_joules, emissions_kg])\n",
        "\n",
        "def log_gpu_details(details, filename=GPU_DETAILS_CSV):\n",
        "    file_exists = os.path.isfile(filename)\n",
        "    with open(filename, 'a', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=details[0].keys())\n",
        "        if not file_exists:\n",
        "            writer.writeheader()\n",
        "        writer.writerows(details)\n",
        "\n",
        "###############################\n",
        "# Load Data\n",
        "###############################\n",
        "df = pd.read_csv('tweets_processed.csv')\n",
        "\n",
        "# Use a subset of data for quicker tests (e.g., 10% of original data)\n",
        "df = df.sample(frac=0.6, random_state=42).reset_index(drop=True)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df['lemmatized_tweets']\n",
        "y = df['VADER_sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
        "\n",
        "###############################\n",
        "# Preprocessing and Tokenization\n",
        "###############################\n",
        "MAX_LENGTH = math.ceil((X_train.apply(lambda x: len(str(x).split())).mean()))+2\n",
        "\n",
        "PAD_TOKEN = \"<|pad|>\"\n",
        "EOS_TOKEN = \"<|endoftext|>\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\",\n",
        "    pad_token=PAD_TOKEN,\n",
        "    eos_token=EOS_TOKEN,\n",
        "    max_length=MAX_LENGTH,\n",
        "    is_split_into_words=True)\n",
        "\n",
        "X_train = [str(ex) + EOS_TOKEN for ex in X_train]\n",
        "X_test = [str(ex) + EOS_TOKEN for ex in X_test]\n",
        "\n",
        "X_train_ = [tokenizer(str(x), return_tensors='tf', max_length=MAX_LENGTH,\n",
        "                      truncation=True, padding=\"max_length\", add_special_tokens=True)['input_ids'] for x in X_train]\n",
        "X_test_ = [tokenizer(str(x), return_tensors='tf', max_length=MAX_LENGTH,\n",
        "                     truncation=True, padding=\"max_length\", add_special_tokens=True)['input_ids'] for x in X_test]\n",
        "\n",
        "X_train_in = tf.squeeze(tf.convert_to_tensor(X_train_), axis=1)\n",
        "X_test_in = tf.squeeze(tf.convert_to_tensor(X_test_), axis=1)\n",
        "\n",
        "X_train_mask_ = [tokenizer(str(x), return_tensors='tf', max_length=MAX_LENGTH,\n",
        "                           truncation=True, padding=\"max_length\", add_special_tokens=True)[\"attention_mask\"] for x in X_train]\n",
        "X_test_mask_ = [tokenizer(str(x), return_tensors='tf', max_length=MAX_LENGTH,\n",
        "                          truncation=True, padding=\"max_length\", add_special_tokens=True)[\"attention_mask\"] for x in X_test]\n",
        "\n",
        "X_train_mask = tf.squeeze(tf.convert_to_tensor(X_train_mask_), axis=1)\n",
        "X_test_mask = tf.squeeze(tf.convert_to_tensor(X_test_mask_), axis=1)\n",
        "\n",
        "###############################\n",
        "# Model Setup\n",
        "###############################\n",
        "model = TFGPT2Model.from_pretrained(\"gpt2\", use_cache=False,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id)\n",
        "model.training = True\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "input_ = tf.keras.layers.Input(shape=(None,), dtype='int32')\n",
        "mask_ = tf.keras.layers.Input(shape=(None,), dtype='int32')\n",
        "x = model(input_, attention_mask=mask_)\n",
        "x = tf.reduce_mean(x.last_hidden_state, axis=1)\n",
        "x = tf.keras.layers.Dense(16, activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(0.3)(x)\n",
        "output = tf.keras.layers.Dense(3, activation='softmax')(x)\n",
        "\n",
        "clf = tf.keras.Model([input_, mask_], output)\n",
        "\n",
        "base_learning_rate = 0.0005\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "clf.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"accuracy\", verbose=1, patience=3, restore_best_weights=True)\n",
        "\n",
        "def map_sentiment(value):\n",
        "  if value == 'Negative':\n",
        "    return 0\n",
        "  if value == 'Neutral':\n",
        "    return 1\n",
        "  if value == 'Positive':\n",
        "    return 2\n",
        "\n",
        "y_train_ = y_train.map(map_sentiment)\n",
        "y_test_ = y_test.map(map_sentiment)\n",
        "\n",
        "y_train_in = tf.constant(y_train_, dtype=tf.int32)\n",
        "y_test_in = tf.constant(y_test_, dtype=tf.int32)\n",
        "\n",
        "tf.config.experimental_run_functions_eagerly(True)\n",
        "\n",
        "def train_model():\n",
        "    clf.fit([X_train_in, X_train_mask], y_train_in, epochs=30, batch_size=32, validation_split=0.2, callbacks=callbacks)\n",
        "\n",
        "###############################\n",
        "# Gather GPU Details\n",
        "###############################\n",
        "gpu_details = get_gpu_details()\n",
        "print(\"GPU Details and Insights:\")\n",
        "for detail in gpu_details:\n",
        "    print(detail)\n",
        "log_gpu_details(gpu_details)\n",
        "\n",
        "###############################\n",
        "# Training with Energy Measurement\n",
        "###############################\n",
        "print(\"Starting training...\")\n",
        "train_time, train_energy_j, train_emissions_kg = measure_energy_and_time(train_model)\n",
        "print(f\"Training completed in {train_time:.2f}s, Energy: {train_energy_j:.2f}J, Emissions: {train_emissions_kg:.4f}kg CO2\")\n",
        "log_results(\"GPT2_TransferLearning\", \"Training\", train_time, train_energy_j, train_emissions_kg)\n",
        "\n",
        "###############################\n",
        "# Evaluation with Energy Measurement\n",
        "###############################\n",
        "print(\"Evaluating on test set...\")\n",
        "eval_time, eval_energy_j, eval_emissions_kg = measure_energy_and_time(clf.evaluate, [X_test_in, X_test_mask], y_test_in)\n",
        "print(f\"Evaluation completed in {eval_time:.2f}s, Energy: {eval_energy_j:.2f}J, Emissions: {eval_emissions_kg:.4f}kg CO2\")\n",
        "log_results(\"GPT2_TransferLearning\", \"Evaluation\", eval_time, eval_energy_j, eval_emissions_kg)\n",
        "\n",
        "clf.training = False\n",
        "y_pred = clf.predict([X_test_in, X_test_mask])\n",
        "\n",
        "print(\"Done. Results saved to:\", RESULTS_CSV)\n"
      ],
      "metadata": {
        "id": "Qv2ePuV7No0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eae608fd-b097-4747-abaa-743957d7015e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (12.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from pynvml) (12.560.30)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2Model.\n",
            "\n",
            "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Details and Insights:\n",
            "{'GPU': 'Tesla T4', 'Memory Total (MB)': 15360, 'Memory Free (MB)': 6793, 'Memory Used (MB)': 8566, 'GPU Utilization (%)': 0, 'Memory Utilization (%)': 0, 'CUDA Version': 12020}\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.8019 - accuracy: 0.3533"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r24/24 [==============================] - 27s 1s/step - loss: 1.8019 - accuracy: 0.3533 - val_loss: 1.0890 - val_accuracy: 0.4392\n",
            "Epoch 2/30\n",
            "24/24 [==============================] - 25s 1s/step - loss: 1.1001 - accuracy: 0.3732 - val_loss: 1.0596 - val_accuracy: 0.4603\n",
            "Epoch 3/30\n",
            "24/24 [==============================] - 25s 1s/step - loss: 1.0729 - accuracy: 0.4170 - val_loss: 1.0744 - val_accuracy: 0.3228\n",
            "Epoch 4/30\n",
            "24/24 [==============================] - 25s 1s/step - loss: 1.0762 - accuracy: 0.3559 - val_loss: 1.0937 - val_accuracy: 0.3228\n",
            "Epoch 5/30\n",
            "24/24 [==============================] - 25s 1s/step - loss: 1.0842 - accuracy: 0.3679 - val_loss: 1.0790 - val_accuracy: 0.3228\n",
            "Epoch 6/30\n",
            "24/24 [==============================] - 26s 1s/step - loss: 1.0688 - accuracy: 0.3665 - val_loss: 1.0775 - val_accuracy: 0.4603\n",
            "Epoch 6: early stopping\n",
            "Restoring model weights from the end of the best epoch: 3.\n",
            "Training completed in 154.58s, Energy: 9198.47J, Emissions: 0.0010kg CO2\n",
            "Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 2s 224ms/step - loss: 1.0654 - accuracy: 0.3599\n",
            "Evaluation completed in 50.29s, Energy: 1819.57J, Emissions: 0.0002kg CO2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 2s 200ms/step\n",
            "Done. Results saved to: energy_measurements_freze_gpu.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"GPT2_Transfer Learning with Energy Measurement and GPU Insights\"\"\"\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install transformers psutil pynvml\n",
        "\n",
        "# Import required libraries\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from transformers import GPT2Tokenizer, TFGPT2Model\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "import time\n",
        "import psutil\n",
        "import threading\n",
        "import csv\n",
        "import os\n",
        "import pynvml\n",
        "\n",
        "###############################\n",
        "# Constants and Setup\n",
        "###############################\n",
        "CARBON_INTENSITY = 0.4  # kg CO2 per kWh (example factor)\n",
        "RESULTS_CSV = \"energy_measurements_gpu.csv\"\n",
        "GPU_DETAILS_CSV = \"gpu_details_gpu.csv\"\n",
        "USE_GPU = tf.config.list_physical_devices('GPU')  # Check if GPU is available\n",
        "\n",
        "###############################\n",
        "# GPU and CPU Power Functions\n",
        "###############################\n",
        "\n",
        "def get_gpu_details():\n",
        "    \"\"\"\n",
        "    Retrieve and print GPU architecture and other insights using NVML.\n",
        "    \"\"\"\n",
        "    pynvml.nvmlInit()\n",
        "    device_count = pynvml.nvmlDeviceGetCount()\n",
        "    details = []\n",
        "\n",
        "    for i in range(device_count):\n",
        "        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
        "        name = pynvml.nvmlDeviceGetName(handle)  # No need to decode\n",
        "        memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
        "        utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
        "        architecture = pynvml.nvmlSystemGetCudaDriverVersion()\n",
        "\n",
        "        details.append({\n",
        "            \"GPU\": name,\n",
        "            \"Memory Total (MB)\": memory_info.total // (1024 ** 2),\n",
        "            \"Memory Free (MB)\": memory_info.free // (1024 ** 2),\n",
        "            \"Memory Used (MB)\": memory_info.used // (1024 ** 2),\n",
        "            \"GPU Utilization (%)\": utilization.gpu,\n",
        "            \"Memory Utilization (%)\": utilization.memory,\n",
        "            \"CUDA Version\": architecture\n",
        "        })\n",
        "\n",
        "    pynvml.nvmlShutdown()\n",
        "    return details\n",
        "\n",
        "\n",
        "# Function to Measure Power Usage with nvidia-smi\n",
        "def get_gpu_power():\n",
        "    if USE_GPU:\n",
        "        result = subprocess.run(\n",
        "            [\"nvidia-smi\", \"--query-gpu=power.draw\", \"--format=csv,nounits,noheader\"],\n",
        "            stdout=subprocess.PIPE, text=True)\n",
        "        return float(result.stdout.strip())\n",
        "    return 0.0\n",
        "\n",
        "# Estimate CPU power usage\n",
        "def get_cpu_power_estimate():\n",
        "    \"\"\"\n",
        "    Estimate CPU power usage.\n",
        "    This is a rough approximation.\n",
        "    \"\"\"\n",
        "    cpu_util = psutil.cpu_percent()\n",
        "    estimated_cpu_power = (cpu_util / 100.0) * 50\n",
        "    return estimated_cpu_power\n",
        "\n",
        "# Measure energy consumption and time\n",
        "def measure_energy_and_time(func, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Measure runtime and approximate energy usage for a given function call.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    energy_gpu_joules = 0.0\n",
        "    energy_cpu_joules = 0.0\n",
        "    interval = 0.1  # Sampling interval in seconds\n",
        "\n",
        "    def target():\n",
        "        func(*args, **kwargs)\n",
        "\n",
        "    thread = threading.Thread(target=target)\n",
        "    thread.start()\n",
        "\n",
        "    while thread.is_alive():\n",
        "        # Measure GPU power using nvidia-smi\n",
        "        gpu_watts = get_gpu_power()\n",
        "        cpu_watts = get_cpu_power_estimate()\n",
        "        time.sleep(interval)\n",
        "        energy_gpu_joules += gpu_watts * interval\n",
        "        energy_cpu_joules += cpu_watts * interval\n",
        "\n",
        "    thread.join()\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    total_energy_joules = energy_gpu_joules + energy_cpu_joules\n",
        "    total_energy_kwh = total_energy_joules * 2.77778e-7\n",
        "    emissions_kg = total_energy_kwh * CARBON_INTENSITY\n",
        "\n",
        "    return total_time, total_energy_joules, emissions_kg\n",
        "\n",
        "# Logging functions\n",
        "def log_results(task_name, phase, total_time, energy_joules, emissions_kg, filename=RESULTS_CSV):\n",
        "    file_exists = os.path.isfile(filename)\n",
        "    with open(filename, 'a', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        if not file_exists:\n",
        "            writer.writerow([\"Task\", \"Phase\", \"Time(s)\", \"Energy(J)\", \"Emissions(kgCO2)\"])\n",
        "        writer.writerow([task_name, phase, total_time, energy_joules, emissions_kg])\n",
        "\n",
        "def log_gpu_details(details, filename=GPU_DETAILS_CSV):\n",
        "    file_exists = os.path.isfile(filename)\n",
        "    with open(filename, 'a', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=details[0].keys())\n",
        "        if not file_exists:\n",
        "            writer.writeheader()\n",
        "        writer.writerows(details)\n",
        "\n",
        "###############################\n",
        "# Load Data\n",
        "###############################\n",
        "df = pd.read_csv('tweets_processed.csv')\n",
        "\n",
        "# Use a subset of data for quicker tests (e.g., 10% of original data)\n",
        "df = df.sample(frac=0.6, random_state=42).reset_index(drop=True)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df['lemmatized_tweets']\n",
        "y = df['VADER_sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
        "\n",
        "###############################\n",
        "# Preprocessing and Tokenization\n",
        "###############################\n",
        "MAX_LENGTH = math.ceil((X_train.apply(lambda x: len(str(x).split())).mean()))+2\n",
        "\n",
        "PAD_TOKEN = \"<|pad|>\"\n",
        "EOS_TOKEN = \"<|endoftext|>\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\",\n",
        "    pad_token=PAD_TOKEN,\n",
        "    eos_token=EOS_TOKEN,\n",
        "    max_length=MAX_LENGTH,\n",
        "    is_split_into_words=True)\n",
        "\n",
        "X_train = [str(ex) + EOS_TOKEN for ex in X_train]\n",
        "X_test = [str(ex) + EOS_TOKEN for ex in X_test]\n",
        "\n",
        "X_train_ = [tokenizer(str(x), return_tensors='tf', max_length=MAX_LENGTH,\n",
        "                      truncation=True, padding=\"max_length\", add_special_tokens=True)['input_ids'] for x in X_train]\n",
        "X_test_ = [tokenizer(str(x), return_tensors='tf', max_length=MAX_LENGTH,\n",
        "                     truncation=True, padding=\"max_length\", add_special_tokens=True)['input_ids'] for x in X_test]\n",
        "\n",
        "X_train_in = tf.squeeze(tf.convert_to_tensor(X_train_), axis=1)\n",
        "X_test_in = tf.squeeze(tf.convert_to_tensor(X_test_), axis=1)\n",
        "\n",
        "X_train_mask_ = [tokenizer(str(x), return_tensors='tf', max_length=MAX_LENGTH,\n",
        "                           truncation=True, padding=\"max_length\", add_special_tokens=True)[\"attention_mask\"] for x in X_train]\n",
        "X_test_mask_ = [tokenizer(str(x), return_tensors='tf', max_length=MAX_LENGTH,\n",
        "                          truncation=True, padding=\"max_length\", add_special_tokens=True)[\"attention_mask\"] for x in X_test]\n",
        "\n",
        "X_train_mask = tf.squeeze(tf.convert_to_tensor(X_train_mask_), axis=1)\n",
        "X_test_mask = tf.squeeze(tf.convert_to_tensor(X_test_mask_), axis=1)\n",
        "\n",
        "###############################\n",
        "# Model Setup\n",
        "###############################\n",
        "model = TFGPT2Model.from_pretrained(\"gpt2\", use_cache=False,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id)\n",
        "model.training = True\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Freeze GPT2 layers\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "input_ = tf.keras.layers.Input(shape=(None,), dtype='int32')\n",
        "mask_ = tf.keras.layers.Input(shape=(None,), dtype='int32')\n",
        "x = model(input_, attention_mask=mask_)\n",
        "x = tf.reduce_mean(x.last_hidden_state, axis=1)\n",
        "x = tf.keras.layers.Dense(16, activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(0.3)(x)\n",
        "output = tf.keras.layers.Dense(3, activation='softmax')(x)\n",
        "\n",
        "clf = tf.keras.Model([input_, mask_], output)\n",
        "\n",
        "base_learning_rate = 0.0005\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "clf.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"accuracy\", verbose=1, patience=3, restore_best_weights=True)\n",
        "\n",
        "def map_sentiment(value):\n",
        "  if value == 'Negative':\n",
        "    return 0\n",
        "  if value == 'Neutral':\n",
        "    return 1\n",
        "  if value == 'Positive':\n",
        "    return 2\n",
        "\n",
        "y_train_ = y_train.map(map_sentiment)\n",
        "y_test_ = y_test.map(map_sentiment)\n",
        "\n",
        "y_train_in = tf.constant(y_train_, dtype=tf.int32)\n",
        "y_test_in = tf.constant(y_test_, dtype=tf.int32)\n",
        "\n",
        "tf.config.experimental_run_functions_eagerly(True)\n",
        "\n",
        "def train_model():\n",
        "    clf.fit([X_train_in, X_train_mask], y_train_in, epochs=30, batch_size=32, validation_split=0.2, callbacks=callbacks)\n",
        "\n",
        "###############################\n",
        "# Gather GPU Details\n",
        "###############################\n",
        "gpu_details = get_gpu_details()\n",
        "print(\"GPU Details and Insights:\")\n",
        "for detail in gpu_details:\n",
        "    print(detail)\n",
        "log_gpu_details(gpu_details)\n",
        "\n",
        "###############################\n",
        "# Training with Energy Measurement\n",
        "###############################\n",
        "print(\"Starting training...\")\n",
        "train_time, train_energy_j, train_emissions_kg = measure_energy_and_time(train_model)\n",
        "print(f\"Training completed in {train_time:.2f}s, Energy: {train_energy_j:.2f}J, Emissions: {train_emissions_kg:.4f}kg CO2\")\n",
        "log_results(\"GPT2_TransferLearning\", \"Training\", train_time, train_energy_j, train_emissions_kg)\n",
        "\n",
        "###############################\n",
        "# Evaluation with Energy Measurement\n",
        "###############################\n",
        "print(\"Evaluating on test set...\")\n",
        "eval_time, eval_energy_j, eval_emissions_kg = measure_energy_and_time(clf.evaluate, [X_test_in, X_test_mask], y_test_in)\n",
        "print(f\"Evaluation completed in {eval_time:.2f}s, Energy: {eval_energy_j:.2f}J, Emissions: {eval_emissions_kg:.4f}kg CO2\")\n",
        "log_results(\"GPT2_TransferLearning\", \"Evaluation\", eval_time, eval_energy_j, eval_emissions_kg)\n",
        "\n",
        "clf.training = False\n",
        "y_pred = clf.predict([X_test_in, X_test_mask])\n",
        "\n",
        "print(\"Done. Results saved to:\", RESULTS_CSV)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8OKDx04NGKD",
        "outputId": "82c2ef74-4a7f-45dd-8abc-b50e2ed726bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (12.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from pynvml) (12.560.30)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2Model.\n",
            "\n",
            "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Details and Insights:\n",
            "{'GPU': 'Tesla T4', 'Memory Total (MB)': 15360, 'Memory Free (MB)': 1037, 'Memory Used (MB)': 14322, 'GPU Utilization (%)': 0, 'Memory Utilization (%)': 0, 'CUDA Version': 12020}\n",
            "Starting training...\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - ETA: 0s - loss: 2.7486 - accuracy: 0.3772"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r24/24 [==============================] - 28s 1s/step - loss: 2.7486 - accuracy: 0.3772 - val_loss: 1.1715 - val_accuracy: 0.4021\n",
            "Epoch 2/30\n",
            "24/24 [==============================] - 15s 641ms/step - loss: 1.3915 - accuracy: 0.4064 - val_loss: 1.0751 - val_accuracy: 0.4550\n",
            "Epoch 3/30\n",
            "24/24 [==============================] - 17s 707ms/step - loss: 1.1530 - accuracy: 0.4409 - val_loss: 1.0388 - val_accuracy: 0.5079\n",
            "Epoch 4/30\n",
            "24/24 [==============================] - 15s 627ms/step - loss: 1.0602 - accuracy: 0.4622 - val_loss: 1.0287 - val_accuracy: 0.5238\n",
            "Epoch 5/30\n",
            "24/24 [==============================] - 15s 616ms/step - loss: 1.0772 - accuracy: 0.4250 - val_loss: 1.0329 - val_accuracy: 0.5291\n",
            "Epoch 6/30\n",
            "24/24 [==============================] - 15s 633ms/step - loss: 1.0402 - accuracy: 0.4768 - val_loss: 1.0315 - val_accuracy: 0.5344\n",
            "Epoch 7/30\n",
            "24/24 [==============================] - 17s 727ms/step - loss: 1.0343 - accuracy: 0.4794 - val_loss: 1.0298 - val_accuracy: 0.5132\n",
            "Epoch 8/30\n",
            "24/24 [==============================] - 17s 710ms/step - loss: 1.0186 - accuracy: 0.5179 - val_loss: 1.0201 - val_accuracy: 0.5291\n",
            "Epoch 9/30\n",
            "24/24 [==============================] - 15s 628ms/step - loss: 1.0254 - accuracy: 0.4847 - val_loss: 1.0161 - val_accuracy: 0.5238\n",
            "Epoch 10/30\n",
            "24/24 [==============================] - 15s 631ms/step - loss: 1.0177 - accuracy: 0.4741 - val_loss: 1.0219 - val_accuracy: 0.5132\n",
            "Epoch 11/30\n",
            "24/24 [==============================] - 15s 642ms/step - loss: 0.9977 - accuracy: 0.5086 - val_loss: 1.0063 - val_accuracy: 0.5608\n",
            "Epoch 11: early stopping\n",
            "Restoring model weights from the end of the best epoch: 8.\n",
            "Training completed in 185.44s, Energy: 11196.96J, Emissions: 0.0012kg CO2\n",
            "Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 2s 220ms/step - loss: 1.0069 - accuracy: 0.5414\n",
            "Evaluation completed in 18.87s, Energy: 711.08J, Emissions: 0.0001kg CO2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 2s 194ms/step\n",
            "Done. Results saved to: energy_measurements_gpu.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Iq28o20ORj4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b363fd8-d5bd-46e8-ca7c-bde7ba69d0d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec 10 13:23:09 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W5kM85MWvOLK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
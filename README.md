# Energy Consumption and Optimization of LLM Models in Training
Authors: Rifat Sadik, SM Enamul Apan

Large Language Models, or LLMs, have become very popular recently. They are widely used for tasks like text generation, dialog generation, and translation. Models like GPT and LLaMA are among the most popular examples. These models are trained on huge datasets and use leverage complex neural networks. While this makes them powerful and efficient, it also means they consume a lot of energy. The objective of this projects are:
* **Develop a method to continuously track GPU and CPU power usage during training and evaluation.**
* **Incorporate emissions data directly into the model development workflow.**

# Published blog
https://medium.com/@rifatsdk/energy-consumption-and-optimization-of-llm-models-in-training-ac2f59a008ac

# Approach

 ## Models:
* GPT-2
   
 ## Platform 
* Google Colab
  
  GPU: Tesla T4
  
  CUDA version: 12.2
  
  NVIDIA-SMI 535.104.05

## Dataset:
Processed tweets

https://github.com/almarengo/gpt2-text-classification/tree/main


 ## Performance metrics: 
* Time
* Energy Consumtion
* Carbon Emission

# Reference list
1. https://github.com/almarengo/gpt2-text-classification/tree/main
2. de Vries, Alex. "The growing energy footprint of artificial intelligence." Joule 7.10 (2023): 2191-2194.
3. Wilkins, G., Keshav, S., & Mortier, R. (2024). Offline energy-optimal llm serving: Workload-based energy models for llm inference on heterogeneous systems. arXiv preprint arXiv:2407.04014.
4. Kachris, C. (2024). A survey on hardware accelerators for large language models. arXiv preprint arXiv:2401.09890.
5. Stojkovic, J., Choukse, E., Zhang, C., Goiri, I., & Torrellas, J. (2024). Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference. arXiv preprint arXiv:2403.20306.



